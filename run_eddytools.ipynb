{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "import xgcm\n",
    "from xorca.lib import load_xorca_dataset\n",
    "import pandas as pd\n",
    "import xesmf as xe\n",
    "from scipy import ndimage\n",
    "from scipy.interpolate import interp1d\n",
    "import pickle\n",
    "import operator\n",
    "import copy\n",
    "import eddytools as et\n",
    "from cmocean import cm\n",
    "import dask\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "datapath = '/scratch/usr/shkifmjr/eddy_tracking'\n",
    "meshpath = glob(datapath + '/1_mesh_mask.nc')\n",
    "data_in = sorted(glob(datapath + '/1_ORION10.L46.LIM2vp.CFCSF6.MOPS.JRA.XIOS2.5-EXP01_5d_20??0101_????1231_grid_[TUV].nc'))\n",
    "data_in_mops = sorted(glob(datapath + '/1_ORION10.L46.LIM2vp.CFCSF6.MOPS.JRA.XIOS2.5-EXP01_5d_20??0101_????1231_mops.nc'))\n",
    "#data_in_tracer = sorted(glob(datapath + '/1_ORION10.L46.LIM2vp.CFCSF6.MOPS.JRA.XIOS2.5-EXP01_5d_200[4-9]0101_????1231_tracer.nc'))\n",
    "data_in_1m = sorted(glob(datapath + '/1_ORION10.L46.LIM2vp.CFCSF6.MOPS.JRA.XIOS2.5-EXP01_1m_20??0101_????1231_grid_T.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define additional variables for xorca\n",
    "mops_vars = {'O2': {'dims': ['t', 'z_c', 'y_c', 'x_c']}, 'PO4': {'dims': ['t', 'z_c', 'y_c', 'x_c']},\n",
    "             'NO3': {'dims': ['t', 'z_c', 'y_c', 'x_c']}, 'DIC': {'dims': ['t', 'z_c', 'y_c', 'x_c']},\n",
    "             'DICP': {'dims': ['t', 'z_c', 'y_c', 'x_c']}, 'ALK': {'dims': ['t', 'z_c', 'y_c', 'x_c']},\n",
    "             'idealpo4': {'dims': ['t', 'z_c', 'y_c', 'x_c']}, 'CFC12': {'dims': ['t', 'z_c', 'y_c', 'x_c']},\n",
    "             'SF6': {'dims': ['t', 'z_c', 'y_c', 'x_c']}, 'co2flux': {'dims': ['t', 'y_c', 'x_c']},\n",
    "             'co2flux_pre': {'dims': ['t', 'y_c', 'x_c']}, 'ph': {'dims': ['t', 'y_c', 'x_c']},\n",
    "             'ph_pre': {'dims': ['t', 'y_c', 'x_c']}, 'fco2': {'dims': ['t', 'y_c', 'x_c']},\n",
    "             'fco2_pre': {'dims': ['t', 'y_c', 'x_c']}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = load_xorca_dataset(data_files=data_in, aux_files=meshpath, model_config='NEST', update_orca_variables = mops_vars,\n",
    "                               input_ds_chunks = {\"time_counter\": 73, \"t\": 73, \"z\": 1, \"deptht\": 1, \"depthu\": 1, \"depthv\": 1, \"depthw\": 1},\n",
    "                               target_ds_chunks = {\"t\": 73, \"z_c\": 1, \"z_l\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mops = load_xorca_dataset(data_files=data_in_mops, aux_files=meshpath, model_config='NEST', update_orca_variables = mops_vars,\n",
    "                               input_ds_chunks = {\"time_counter\": 73, \"t\": 73, \"z\": 1, \"deptht\": 1, \"depthu\": 1, \"depthv\": 1, \"depthw\": 1},\n",
    "                               target_ds_chunks = {\"t\": 73, \"z_c\": 1, \"z_l\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tracer = load_xorca_dataset(data_files=data_in_tracer, aux_files=meshpath, model_config='NEST', update_orca_variables = mops_vars,\n",
    "                               input_ds_chunks = {\"time_counter\": 73, \"t\": 73, \"z\": 1, \"deptht\": 1, \"depthu\": 1, \"depthv\": 1, \"depthw\": 1},\n",
    "                               target_ds_chunks = {\"t\": 73, \"z_c\": 1, \"z_l\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1m = load_xorca_dataset(data_files=data_in_1m, aux_files=meshpath, model_config='NEST', update_orca_variables = mops_vars,\n",
    "                             input_ds_chunks = {\"time_counter\": 12, \"t\": 12, \"z\": 1, \"deptht\": 1, \"depthu\": 1, \"depthv\": 1, \"depthw\": 1},\n",
    "                             target_ds_chunks = {\"t\": 12, \"z_c\": 1, \"z_l\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metrics for xgcm\n",
    "at, au, av, af = data['e1t'] * data['e2t'], data['e1u'] * data['e2u'], data['e1v'] * data['e2v'], data['e1f'] * data['e2f']\n",
    "vt, vu, vv, vw = data['e3t'] * at, data['e3u'] * au, data['e3v'] * av, data['e3w'] * at\n",
    "\n",
    "data = data.update({'at': at, 'au': au, 'av': av, 'af': af, 'vt': vt, 'vu': vu, 'vv': vv, 'vw': vw})\n",
    "data = data.set_coords(['at', 'au', 'av', 'af', 'vt', 'vu', 'vv', 'vw'])\n",
    "\n",
    "metrics = {\n",
    "    ('X',): ['e1t', 'e1u', 'e1v', 'e1f'], # X distances\n",
    "    ('Y',): ['e2t', 'e2u', 'e2v', 'e2f'], # Y distances\n",
    "    ('Z',): ['e3t', 'e3u', 'e3v', 'e3w'], # Z distances\n",
    "    ('X', 'Y'): ['at', 'au', 'av', 'af'], # Areas\n",
    "    ('X', 'Y', 'Z'): ['vt', 'vu', 'vv', 'vw'] # Volumes\n",
    "}\n",
    "\n",
    "metrics_noZ = {\n",
    "    ('X',): ['e1t', 'e1u', 'e1v', 'e1f'], # X distances\n",
    "    ('Y',): ['e2t', 'e2u', 'e2v', 'e2f'], # Y distances\n",
    "    ('X', 'Y'): ['at', 'au', 'av', 'af'], # Areas\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add bathymetry to data to have depth information\n",
    "bathy = xr.open_dataset('/scratch/usr/shkifmjr/NUSERDATA/ORION/10-data/bathy_meter/1_bathy_meter__3.6.0_ORION10.L46_Kv1.0.0.nc')\n",
    "data = data.update({'bathymetry': (['y_c', 'x_c'], bathy['Bathymetry'].data)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = xr.merge([data, data_mops])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OKUBO-WEISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = xgcm.Grid(data, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_OW = et.okuboweiss.calc(data, grid, 'vozocrtx', 'vomecrty').chunk({'x_c': -1, 'x_r': -1, 'y_c': -1, 'y_r': -1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTERPOLATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate variables to a regular grid to simplify calculations\n",
    "interpolation_parameters = {'start_time': '2000-01-01', # time range start\n",
    "                            'end_time': '2018-12-31', # time range end\n",
    "                            'lon1': np.floor(data['llon_cc'][0,0].values), # minimum longitude of detection region\n",
    "                            'lon2': np.ceil(data['llon_cc'][0,-1].values + 360), # maximum longitude\n",
    "                            'lat1': np.floor(data['llat_cc'].min().values), # minimum latitude\n",
    "                            'lat2': np.ceil(data['llat_cc'].max().values), # maximum latitude\n",
    "                            'res': 1./10., # resolution of the fields\n",
    "                            'vars_to_interpolate': ['OW', 'vort'], # variables to be interpolated \n",
    "                            'mask_to_interpolate': ['fmask', 'tmask', 'bathymetry']} # mask to interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_int_OW = et.interp.horizontal(data_OW.isel(z_c=9, z_l=9), interpolation_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save interpolated file so the calculations above do not have to be redone!\n",
    "data_int_OW.to_netcdf(datapath + '/1_ORION10.L46.LIM2vp.CFCSF6.MOPS.JRA.XIOS2.5-EXP01_5d_20000101_20181231_OW_interpolated_k10.nc', \n",
    "                   mode='w', format='NETCDF4_CLASSIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load interpolated file if it has been saved before\n",
    "data_int = xr.open_dataset(datapath + '/1_ORION10.L46.LIM2vp.CFCSF6.MOPS.JRA.XIOS2.5-EXP01_5d_20000101_20181231_OW_interpolated_k10.nc',\n",
    "                           chunks={'time': 1, 'lon': 510, 'lat': 200})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_int_T = xr.open_mfdataset(sorted(glob(datapath\n",
    "                             + '/interpol/1_ORION10.L46.LIM2vp.CFCSF6.MOPS.JRA.XIOS2.5-EXP01_REG10_5d_20??0101_20??1231_grid_T.nc')),\n",
    "                           chunks={'time': 1, 'lon': 510, 'lat': 200})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_int_mops = xr.open_mfdataset(sorted(glob(datapath \n",
    "                                + '/interpol/1_ORION10.L46.LIM2vp.CFCSF6.MOPS.JRA.XIOS2.5-EXP01_REG10_5d_20??0101_20??1231_mops.nc')),\n",
    "                           chunks={'time': 1, 'lon': 510, 'lat': 200})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_int = xr.merge([data_int, data_int_T, data_int_mops])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_OW_spatial_std = xr.open_dataset(datapath + '/1_ORION10.L46.LIM2vp.CFCSF6.MOPS.JRA.XIOS2.5-EXP01_5d_20000101_20181231_mean_OW_std.nc',\n",
    "                           chunks={'lon': 510, 'lat': 200})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OW_tmp = data_int['OW'].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OW_tmp = OW_tmp.where(OW_tmp != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_OW_spatial_std = OW_tmp.rolling(lon=100, center=True, min_periods=1).std(skipna=True).rolling(lat=100, center=True, min_periods=1).std(skipna=True).mean('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_OW_spatial_std.to_dataset(name='OW_std').to_netcdf(datapath + \n",
    "        '/1_ORION10.L46.LIM2vp.CFCSF6.MOPS.JRA.XIOS2.5-EXP01_5d_20000101_20181231_mean_OW_std.nc', format='NETCDF4_CLASSIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_int = xr.merge([data_int, mean_OW_spatial_std]).chunk({'time': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify parameters for eddy detection\n",
    "detection_parameters = {'start_time': '2000-01-01', # time range start\n",
    "                        'end_time': '2018-12-31', # time range end\n",
    "                        'lon1': np.floor(data_int['lon'][0].values), # minimum longitude of detection region\n",
    "                        'lon2': np.ceil(data_int['lon'][-1].values), # maximum longitude\n",
    "                        'lat1': np.floor(data_int['lat'].min().values), # minimum latitude\n",
    "                        'lat2': np.ceil(data_int['lat'].max().values), # maximum latitude\n",
    "                        'min_dep': 1000, # minimum ocean depth where to look for eddies\n",
    "                        'res': 1./10., # resolution of the fields\n",
    "                        'OW_thr': -0.0001, # \n",
    "                        'OW_thr_name': 'OW_std', #.compute(), # Okubo-Weiss threshold for eddy detection\n",
    "                        'OW_thr_factor': -0.3,\n",
    "                        'Npix_min': 15, # minimum number of pixels (grid cells) to be considered as eddy\n",
    "                        'Npix_max': 2000} # maximum number of pixels (grid cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eddies = et.detection.detect(data_int.isel(depth=9), detection_parameters, 'OW', 'vort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eddies_list = []\n",
    "for i in np.arange(0, len(eddies)):\n",
    "    eddies_list.append(eddies[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(0, len(eddies_list)):\n",
    "    datestring = str(eddies_list[i][0]['time'])[0:10]\n",
    "    with open('/scratch/usr/shkifmjr/eddy_tracking/eddies/'\n",
    "          + '1_ORION10.L46.LIM2vp.CFCSF6.MOPS.JRA.XIOS2.5-EXP01_5d_' + str(datestring) + '_eddies_OW0.3.pickle', 'wb') as f:\n",
    "        pickle.dump(eddies_list[i], f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRACKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify parameters for eddy tracking\n",
    "tracking_parameters = {'start_time': '2000-01-03', # time range start\n",
    "                        'end_time': '2018-12-31', # time range end\n",
    "                        'dt': 5,\n",
    "                        'lon1': np.floor(data_int['lon'][0].values), # minimum longitude of detection region\n",
    "                        'lon2': np.ceil(data_int['lon'][-1].values), # maximum longitude\n",
    "                        'lat1': np.floor(data_int['lat'].min().values), # minimum latitude\n",
    "                        'lat2': np.ceil(data_int['lat'].max().values), # maximum latitude\n",
    "                       'dE': 0., # maximum distance of search ellipsis from eddy center in towards the east \n",
    "                                 # (if set to 0, it will be calculated as (150. / (7. / dt)))\n",
    "                       'eddy_scale_min': 0.5, # minimum factor by which eddy amplitude and area are allowed to change in one timestep\n",
    "                       'eddy_scale_max': 1.5, # maximum factor by which eddy amplitude and area are allowed to change in one timestep\n",
    "                       'data_path': datapath + '/', # path to the detected eddies pickle files\n",
    "                       'file_root': 'eddies/1_ORION10.L46.LIM2vp.CFCSF6.MOPS.JRA.XIOS2.5-EXP01_5d',\n",
    "                       'file_spec': 'eddies_OW0.3',\n",
    "                       'ross_path': datapath + '/'} # path to rossrad.dat containing Chelton et 1. 1998 Rossby radii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking at time step  155  of  1387\n",
      "tracking at time step  309  of  1387\n",
      "tracking at time step  463  of  1387\n",
      "tracking at time step  617  of  1387\n",
      "tracking at time step  772  of  1387\n",
      "tracking at time step  926  of  1387\n",
      "tracking at time step  1080  of  1387\n",
      "tracking at time step  1234  of  1387\n"
     ]
    }
   ],
   "source": [
    "tracks = et.tracking.track(tracking_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/scratch/usr/shkifmjr/eddy_tracking/tracks/'\n",
    "          + '1_ORION10.L46.LIM2vp.CFCSF6.MOPS.JRA.XIOS2.5-EXP01_5d_20000101_20181231_tracks_OW0.3.pickle', 'wb') as f:\n",
    "    pickle.dump(tracks, f, pickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/scratch/usr/shkifmjr/eddy_tracking/'\n",
    "          + '1_ORION10.L46.LIM2vp.CFCSF6.MOPS.JRA.XIOS2.5-EXP01_5d_20000101_20091231_tracks_OW0.3.pickle', 'rb') as f:\n",
    "    tracks = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_parameters = {'start_time': '2000-01-01', # time range start\n",
    "                     'end_time': '2000-12-31', # time range end\n",
    "                     'lon1': -180, #np.floor(data_int['lon'][0].values), # minimum longitude of detection region\n",
    "                     'lon2': 180, #np.ceil(data_int['lon'][-1].values), # maximum longitude\n",
    "                     'lat1': np.floor(data_int['lat'].min().values), # minimum latitude\n",
    "                     'lat2': np.ceil(data_int['lat'].max().values), # maximum latitude\n",
    "                     'type': 'anticyclonic', # this is cyclonic due to error in detection\n",
    "                     'lifetime': 30, # length of the eddy's track in days\n",
    "                     'size': 25, # eddy size (diameter in km)\n",
    "                     'range': False,\n",
    "                     'ds_range': 0, #data_int.isel(depth=9).chunk({'lat': 50, 'lon': 50, 'time': 1}), \n",
    "                     'var_range': ['votemper'],\n",
    "                     'value_range': [[-2., 15.]], #[[-1.2, -0.1],], # only sample eddies within this mean var_range range\n",
    "                     'split': False,\n",
    "                     'ds_split': 0, #data_int.isel(depth=9).chunk({'lat': 50, 'lon': 50, 'time': 1}),\n",
    "                     'var_split': ['DIC'],\n",
    "                     'value_split': [2125,], # split eddies at this value of var_split\n",
    "                     'sample_vars': ['votemper', 'DIC']\n",
    "                      } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = tracks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f add_fields add_fields(test_sample, data_int_cunked, 'votemper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fields(sampled, interpolated, var):\n",
    "    # Initialize additional dictionary entries in which to write the fields\n",
    "    sampled[var] = {}\n",
    "    sampled[var + '_lon'] = {}\n",
    "    sampled[var + '_lat'] = {}\n",
    "    sampled[var + '_sec'] = {}\n",
    "    sampled[var + '_sec_lon'] = {}\n",
    "    sampled[var + '_sec_lat'] = {}\n",
    "    sampled[var + '_around'] = {}\n",
    "    sampled[var + '_sec_norm_lon'] = {}\n",
    "    try:\n",
    "        length = len(sampled['time'])\n",
    "    except:\n",
    "        length = 1\n",
    "    for t in np.arange(0, length):\n",
    "        # loop over all time steps of the eddy track\n",
    "        t = int(t)\n",
    "        if length == 1:\n",
    "            time = sampled['time']\n",
    "        else:\n",
    "            time = sampled['time'][t]\n",
    "        # get the indeces to use for extraction from `interpolated`\n",
    "        indeces = np.vstack((sampled['eddy_i'][t],\n",
    "                             sampled['eddy_j'][t]))\n",
    "        t_index = np.min(np.where(interpolated['time'].values >= time))\n",
    "        # add the variable `var` and its coordinates inside the eddy to\n",
    "        # `sampled[i]`\n",
    "        sampled[var][t] = interpolated[var][t_index, :,\n",
    "                                               indeces[1, :], indeces[0, :]]\n",
    "        sampled[var + '_lon'][t] = sampled[var][t]['lon'].values\n",
    "        sampled[var + '_lat'][t] = sampled[var][t]['lat'].values\n",
    "        # add the values of `var` along a zonal section through the middle of\n",
    "        # the eddy, together with the coordinates\n",
    "        sampled[var + '_sec'][t] =\\\n",
    "            interpolated[var][t_index, :, int(np.mean(indeces[1, :])),\n",
    "                              np.min(indeces[0, :]):np.max(indeces[0, :])]\n",
    "        sampled[var + '_sec_lon'][t] =\\\n",
    "            sampled[var + '_sec'][t]['lon'].values\n",
    "        sampled[var + '_sec_lat'][t] =\\\n",
    "            sampled[var + '_sec'][t]['lat'].values\n",
    "        # normalize longitude to the range (-0.5, 0.5) for easier comparison\n",
    "        # of different eddies\n",
    "        diff_lon = (sampled[var + '_sec'][t]['lon']\n",
    "                    - sampled[var + '_sec'][t]['lon'].mean())\n",
    "        norm_lon = diff_lon / (diff_lon[-1] - diff_lon[0])\n",
    "        sampled[var + '_sec_norm_lon'][t] = norm_lon.values\n",
    "        sampled[var + '_sec'][t] = sampled[var + '_sec'][t].values\n",
    "        # add a depth profile of the values of `var` in the surroundings of\n",
    "        # the eddy to calculate anomalies\n",
    "        sampled[var + '_around'][t] =\\\n",
    "            average_surroundings(indeces, interpolated[var], t_index)\n",
    "        sampled[var][t] = sampled[var][t].values\n",
    "    return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_surroundings(indeces, interpolated, t_index):\n",
    "    # Calculate the radius of the eddy in \"index space\"\n",
    "    radius = int(((np.max(indeces[0, :]) - np.min(indeces[0, :])) / 2))\n",
    "    # add one radiues in each direction to define what are the surroundings\n",
    "    imin = np.min(indeces[0, :]) - radius\n",
    "    imax = np.max(indeces[0, :]) + radius + 1\n",
    "    jmin = np.min(indeces[1, :]) - radius\n",
    "    jmax = np.max(indeces[1, :]) + radius + 1\n",
    "    reduced = interpolated[t_index, ...].copy().squeeze().values\n",
    "    # Set the values inside the eddy to NaN because we only want the\n",
    "    # surroundings\n",
    "    for j in np.arange(len(indeces[0, :])):\n",
    "        reduced[:, indeces[1, j], indeces[0, j]] = np.nan\n",
    "\n",
    "    reduced = np.where(reduced == 0, np.nan, reduced)\n",
    "    # Select the region around the eddy (+- one radius) and average\n",
    "    around = np.nanmean(reduced[:, jmin:jmax, imin:imax], axis=(1, 2))\n",
    "    return around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monotonic_lon(data, lon):\n",
    "    # Make sure the longitude is monotonically increasing for the interpolation\n",
    "    if data['lon'][0] > data['lon'][-1]:\n",
    "        lon_mod = data['lon']\\\n",
    "            .where(data['lon']\n",
    "                   >= np.around(data['lon'][0].values),\n",
    "                   other=data['lon'] + 360)\n",
    "        data['lon'].values = lon_mod.values\n",
    "        if lon < lon_mod[0]:\n",
    "            lon = lon + 360\n",
    "    return data, lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_param = sample_parameters.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def sample(tracks, data, sample_param):\n",
    "# Initialize depending on whether `split`: True or False\n",
    "if sample_param['split']:\n",
    "    below = {}\n",
    "    above = {}\n",
    "    i = 0\n",
    "    j = 0\n",
    "else:\n",
    "    sampled = {}\n",
    "    i = 0\n",
    "    j = 0 \n",
    "# Try to detect time step of eddy tracks in days\n",
    "try:\n",
    "    timestep = ((tracks[0]['time'][1] - tracks[0]['time'][0])\n",
    "                / np.timedelta64(1, 'D'))\n",
    "except:\n",
    "    k = 0\n",
    "    while len(tracks[k]['time']) < 2:\n",
    "        k = k + 1\n",
    "    timestep = ((tracks[k]['time'][1] - tracks[k]['time'][0])\n",
    "                / np.timedelta64(1, 'D'))\n",
    "# Convert lifetime from days to indeces\n",
    "lifetime = sample_param['lifetime'] / timestep\n",
    "start_time = np.datetime64(sample_param['start_time'])\n",
    "end_time = np.datetime64(sample_param['end_time'])\n",
    "\n",
    "    #if sample_param['split']:\n",
    "    #    for ed in np.arange(0, len(tracks)):\n",
    "    #        below[ed], above[ed] = sample_core(tracks[ed], sample_parameters)\n",
    "    #    return below, above\n",
    "    #else:\n",
    "    #    for ed in np.arange(0, len(tracks)):\n",
    "    #        sampled[ed] = sample_core(tracks[ed], sample_parameters)\n",
    "   #     return sampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_core(tracks, data, sample_param, i, j, lifetime, start_time, end_time):\n",
    "    if sample_param['split']:\n",
    "        below = {}\n",
    "        above = {}\n",
    "    else:\n",
    "        sampled = {}\n",
    "    # loop over all eddies in `tracks`\n",
    "    # determine first and last time step of each eddy\n",
    "    try:\n",
    "        length = len(tracks['time'])\n",
    "        time0 = np.array(tracks['time'])[0]\n",
    "        time1 = np.array(tracks['time'])[-1]\n",
    "    except:\n",
    "        length = 1\n",
    "        time0 = np.array(tracks['time'])\n",
    "        time1 = np.array(tracks['time'])\n",
    "    # determine lon and lat of eddy\n",
    "    lon_for_sel = tracks['lon'][0]\n",
    "    if lon_for_sel > 180.:\n",
    "        lon_for_sel = lon_for_sel - 360.\n",
    "    lat_for_sel = tracks['lat'][0]\n",
    "    # if necessary extract values of of `var_range` and `var_split` at the\n",
    "    # eddy's location\n",
    "    if sample_param['range']:\n",
    "        data_range = sample_param['ds_range']\n",
    "        data_range, lon_for_sel = monotonic_lon(data_range, lon_for_sel)\n",
    "        vars_range_ed =\\\n",
    "            data_range[sample_param['var_range'][0]]\\\n",
    "            .sel(time=time0, lat=lat_for_sel,\n",
    "                 lon=lon_for_sel, method='nearest').values\n",
    "    if sample_param['split']:\n",
    "        data_split = sample_param['ds_split']\n",
    "        data_split, lon_for_sel = monotonic_lon(data_split, lon_for_sel)\n",
    "        vars_split_ed =\\\n",
    "            data_split[sample_param['var_split'][0]]\\\n",
    "            .sel(time=time0, lat=lat_for_sel,\n",
    "                 lon=lon_for_sel, method='nearest').values\n",
    "    # construct bool of all conditions to be met by the eddy to be\n",
    "    # considered in `sampled`\n",
    "    conditions_are_met = (\n",
    "        (time0 >= start_time)\n",
    "         & (time1 <= end_time)\n",
    "         & (tracks['type'] == sample_param['type'])\n",
    "         & (length > lifetime)\n",
    "         & (tracks['scale'].mean() > sample_param['size'])\n",
    "         & (tracks['lon'][0] > sample_param['lon1'])\n",
    "         & (tracks['lon'][0] < sample_param['lon2'])\n",
    "         & (tracks['lat'][0] > sample_param['lat1'])\n",
    "         & (tracks['lat'][0] < sample_param['lat2']))\n",
    "    # add conditions for `range` and `split` if necessary and then add\n",
    "    # the required fields to the eddy dictionary\n",
    "    if sample_param['range']:\n",
    "        conditions_are_met = (\n",
    "            conditions_are_met\n",
    "            & (vars_range_ed > sample_param['value_range'][0][0])\n",
    "            & (vars_range_ed < sample_param['value_range'][0][1]))\n",
    "    if sample_param['split']:\n",
    "        if conditions_are_met & (vars_split_ed\n",
    "                                 < sample_param['value_split'][0]):\n",
    "            below = tracks.copy()\n",
    "            for variable in sample_param['sample_vars']:\n",
    "                below = add_fields(below, data, variable)\n",
    "            i = i + 1\n",
    "        elif conditions_are_met & (vars_split_ed\n",
    "                                   >= sample_param['value_split'][0]):\n",
    "            above = tracks.copy()\n",
    "            for variable in sample_param['sample_vars']:\n",
    "                above = add_fields(above, data, variable)\n",
    "            j = j + 1\n",
    "    else:\n",
    "        if conditions_are_met:\n",
    "            sampled = tracks.copy()\n",
    "            for variable in sample_param['sample_vars']:\n",
    "                sampled = add_fields(sampled, data, variable)\n",
    "            i = i + 1\n",
    "    if sample_param['split']:\n",
    "        return below, above, i, j\n",
    "    else:\n",
    "        return sampled, i, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_int_cunked = data_int.chunk({'lat': 50, 'lon': 50, 'time': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f sample_core sample_core(tracks[4], data_int_cunked, sample_parameters, i, j, lifetime, start_time, end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled = et.sample.sample(tracks, data_int.chunk({'lat': 50, 'lon': 50, 'time': 1}), sample_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(above[0]['vosaline_sec_lon'][5], np.arange(0,46), above[0]['vosaline_sec'][5], vmin=34, vmax=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(above[0]['vosaline_sec_lon'][5], np.arange(0,46), above[0]['vosaline_sec'][5] - above[0]['vosaline_around'][5][:,None], \n",
    "               vmin=-0.04, vmax=0.04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parallel execution of loops in sample.py????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fields(sampled, interpolated, i, var):\n",
    "    # Initialize additional dictionary entries in which to write the fields\n",
    "    sampled[i][var] = {}\n",
    "    sampled[i][var + '_lon'] = {}\n",
    "    sampled[i][var + '_lat'] = {}\n",
    "    sampled[i][var + '_sec'] = {}\n",
    "    sampled[i][var + '_sec_lon'] = {}\n",
    "    sampled[i][var + '_sec_lat'] = {}\n",
    "    sampled[i][var + '_around'] = {}\n",
    "    sampled[i][var + '_sec_norm_lon'] = {}\n",
    "    try:\n",
    "        length = len(sampled[i]['time'])\n",
    "    except:\n",
    "        length = 1\n",
    "    for t in np.arange(0, length):\n",
    "        # loop over all time steps of the eddy track\n",
    "        t = int(t)\n",
    "        if length == 1:\n",
    "            time = sampled[i]['time']\n",
    "        else:\n",
    "            time = sampled[i]['time'][t]\n",
    "        # get the indeces to use for extraction from `interpolated`\n",
    "        indeces = np.vstack((sampled[i]['eddy_i'][t],\n",
    "                             sampled[i]['eddy_j'][t]))\n",
    "        t_index = np.min(np.where(interpolated['time'].values >= time))\n",
    "        # add the variable `var` and its coordinates inside the eddy to\n",
    "        # `sampled[i]`\n",
    "        sampled[i][var][t] = interpolated[var][t_index, :,\n",
    "                                               indeces[1, :], indeces[0, :]]\n",
    "        sampled[i][var + '_lon'][t] = sampled[i][var][t]['lon'].values\n",
    "        sampled[i][var + '_lat'][t] = sampled[i][var][t]['lat'].values\n",
    "        # add the values of `var` along a zonal section through the middle of\n",
    "        # the eddy, together with the coordinates\n",
    "        sampled[i][var + '_sec'][t] =\\\n",
    "            interpolated[var][t_index, :, int(np.mean(indeces[1, :])),\n",
    "                              np.min(indeces[0, :]):np.max(indeces[0, :])]\n",
    "        sampled[i][var + '_sec_lon'][t] =\\\n",
    "            sampled[i][var + '_sec'][t]['lon'].values\n",
    "        sampled[i][var + '_sec_lat'][t] =\\\n",
    "            sampled[i][var + '_sec'][t]['lat'].values\n",
    "        # normalize longitude to the range (-0.5, 0.5) for easier comparison\n",
    "        # of different eddies\n",
    "        diff_lon = (sampled[i][var + '_sec'][t]['lon']\n",
    "                    - sampled[i][var + '_sec'][t]['lon'].mean())\n",
    "        norm_lon = diff_lon / (diff_lon[-1] - diff_lon[0])\n",
    "        sampled[i][var + '_sec_norm_lon'][t] = norm_lon.values\n",
    "        sampled[i][var + '_sec'][t] = sampled[i][var + '_sec'][t].values\n",
    "        # add a depth profile of the values of `var` in the surroundings of\n",
    "        # the eddy to calculate anomalies\n",
    "        sampled[i][var + '_around'][t] =\\\n",
    "            average_surroundings(indeces, interpolated[var], t_index)\n",
    "        sampled[i][var][t] = sampled[i][var][t].values\n",
    "    return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_surroundings(indeces, interpolated, t_index):\n",
    "    # Calculate the radius of the eddy in \"index space\"\n",
    "    radius = int(((np.max(indeces[0, :]) - np.min(indeces[0, :])) / 2))\n",
    "    # add one radiues in each direction to define what are the surroundings\n",
    "    imin = np.min(indeces[0, :]) - radius\n",
    "    imax = np.max(indeces[0, :]) + radius + 1\n",
    "    jmin = np.min(indeces[1, :]) - radius\n",
    "    jmax = np.max(indeces[1, :]) + radius + 1\n",
    "    reduced = interpolated[t_index, ...].copy().squeeze().values\n",
    "    # Set the values inside the eddy to NaN because we only want the\n",
    "    # surroundings\n",
    "    for j in np.arange(len(indeces[0, :])):\n",
    "        reduced[:, indeces[1, j], indeces[0, j]] = np.nan\n",
    "\n",
    "    reduced = np.where(reduced == 0, np.nan, reduced)\n",
    "    # Select the region around the eddy (+- one radius) and average\n",
    "    around = np.nanmean(reduced[:, jmin:jmax, imin:imax], axis=(1, 2))\n",
    "    return around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monotonic_lon(data, lon):\n",
    "    if data['lon'][0] > data['lon'][-1]:\n",
    "        lon_mod = data['lon']\\\n",
    "            .where(data['lon']\n",
    "                   >= np.around(data['lon'][0].values),\n",
    "                   other=data['lon'] + 360)\n",
    "        data['lon'].values = lon_mod.values\n",
    "        if lon < lon_mod[0]:\n",
    "            lon = lon + 360\n",
    "    return data, lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocess import Process, Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(tracks, data, sample_param):\n",
    "    if sample_param['split']:\n",
    "        below = {}\n",
    "        above = {}\n",
    "        i = 0\n",
    "        j = 0\n",
    "    else:\n",
    "        sampled = {}\n",
    "        i = 0\n",
    "    # Try to detect time step of eddy tracks in days\n",
    "    try:\n",
    "        timestep = ((tracks[0]['time'][1] - tracks[0]['time'][0])\n",
    "                    / np.timedelta64(1, 'D'))\n",
    "    except:\n",
    "        k = 0\n",
    "        while len(tracks[k]['time']) < 2:\n",
    "            k = k + 1\n",
    "        timestep = ((tracks[k]['time'][1] - tracks[k]['time'][0])\n",
    "                    / np.timedelta64(1, 'D'))\n",
    "    # Convert lifetime from days to indeces\n",
    "    lifetime = sample_param['lifetime'] / timestep\n",
    "    start_time = np.datetime64(sample_param['start_time'])\n",
    "    end_time = np.datetime64(sample_param['end_time'])\n",
    "    data = data.sel(time=slice(sample_param['start_time'],\n",
    "                               sample_param['end_time']))\n",
    "    \n",
    "    manager = Manager()\n",
    "    sampled = manager.dict()\n",
    "    job = [Process(target=sample_core, args=(tracks, data, sample_param, lifetime,\n",
    "                                             start_time, end_time, ed, i, sampled=sampled)) for ed in np.arange(0, len(tracks))]\n",
    "    _ = [p.start() for p in job]\n",
    "    _ = [p.join() for p in job]\n",
    "        \n",
    "    if sample_param['split']:\n",
    "        return below, above\n",
    "    else:\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_core(tracks, data, sample_param, lifetime, start_time, end_time, ed, i, j=None, sampled=None, above=None, below=None):\n",
    "    # loop over all eddies in `tracks`\n",
    "    # determine first and last time step of each eddy\n",
    "    try:\n",
    "        length = len(tracks[ed]['time'])\n",
    "        time0 = np.array(tracks[ed]['time'])[0]\n",
    "        time1 = np.array(tracks[ed]['time'])[-1]\n",
    "    except:\n",
    "        length = 1\n",
    "        time0 = np.array(tracks[ed]['time'])\n",
    "        time1 = np.array(tracks[ed]['time'])\n",
    "    # determine lon and lat of eddy\n",
    "    lon_for_sel = tracks[ed]['lon'][0]\n",
    "    if lon_for_sel > 180.:\n",
    "        lon_for_sel = lon_for_sel - 360.\n",
    "    lat_for_sel = tracks[ed]['lat'][0]\n",
    "    # if necessary extract values of of `var_range` and `var_split` at the\n",
    "    # eddy's location\n",
    "    if sample_param['range']:\n",
    "        data_range = sample_param['ds_range']\n",
    "        data_range, lon_for_sel = monotonic_lon(data_range, lon_for_sel)\n",
    "        vars_range_ed =\\\n",
    "            data_range[sample_param['var_range'][0]]\\\n",
    "            .sel(time=time0, lat=lat_for_sel,\n",
    "                 lon=lon_for_sel, method='nearest').values\n",
    "    if sample_param['split']:\n",
    "        data_split = sample_param['ds_split']\n",
    "        data_split, lon_for_sel = monotonic_lon(data_split, lon_for_sel)\n",
    "        vars_split_ed =\\\n",
    "            data_split[sample_param['var_split'][0]]\\\n",
    "            .sel(time=time0, lat=lat_for_sel,\n",
    "                 lon=lon_for_sel, method='nearest').values\n",
    "    # construct bool of all conditions to be met by the eddy to be\n",
    "    # considered in `sampled`\n",
    "    conditions_are_met = (\n",
    "        (time0 >= start_time)\n",
    "        & (time1 <= end_time)\n",
    "        & (tracks[ed]['type'] == sample_param['type'])\n",
    "        & (length > lifetime)\n",
    "        & (np.mean(tracks[ed]['scale']) > sample_param['size'])\n",
    "        & (tracks[ed]['lon'][0] > sample_param['lon1'])\n",
    "        & (tracks[ed]['lon'][0] < sample_param['lon2'])\n",
    "        & (tracks[ed]['lat'][0] > sample_param['lat1'])\n",
    "        & (tracks[ed]['lat'][0] < sample_param['lat2']))\n",
    "    # add conditions for `range` and `split` if necessary and then add\n",
    "    # the required fields to the eddy dictionary\n",
    "    if sample_param['range']:\n",
    "        conditions_are_met = (\n",
    "            conditions_are_met\n",
    "            & (vars_range_ed > sample_param['value_range'][0][0])\n",
    "            & (vars_range_ed < sample_param['value_range'][0][1]))\n",
    "    if sample_param['split']:\n",
    "        if conditions_are_met & (vars_split_ed\n",
    "                                 < sample_param['value_split'][0]):\n",
    "            below[i] = tracks[ed].copy()\n",
    "            for variable in sample_param['sample_vars']:\n",
    "                below = add_fields(below, data, i, variable)\n",
    "            i = i + 1\n",
    "        elif conditions_are_met & (vars_split_ed\n",
    "                                   >= sample_param['value_split'][0]):\n",
    "            above[j] = tracks[ed].copy()\n",
    "            for variable in sample_param['sample_vars']:\n",
    "                above = add_fields(above, data, j, variable)\n",
    "            j = j + 1\n",
    "    else:\n",
    "        if conditions_are_met:\n",
    "                sampled[i] = tracks[ed].copy()\n",
    "                for variable in sample_param['sample_vars']:\n",
    "                    sampled = add_fields(sampled, data, i, variable)\n",
    "                i = i + 1\n",
    "    \n",
    "    if sample_param['split']:\n",
    "        return below, above\n",
    "    else:\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AVERAGING &#9744;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(sampled, vars, z):\n",
    "    lon_interp = np.arange(-0.5, 0.51, 0.01)\n",
    "    max_time = 0\n",
    "    for ed in np.arange(0, len(sampled)):\n",
    "        if len(sampled[ed]['time']) > max_time:\n",
    "            max_time = len(sampled[ed]['time'])\n",
    "    aves = {}\n",
    "    for v in vars:\n",
    "        aves[v] = {}\n",
    "        for ed in np.arange(0, len(sampled)):\n",
    "            month = str(sampled[ed]['time'][0])[5:7]\n",
    "            try:\n",
    "                aves[v][month] = np.vstack((aves[v][month], np.zeros((1, max_time, len(z), len(lon_interp))) + np.nan))\n",
    "            except:\n",
    "                aves[v][month] = np.zeros((1, max_time, len(z), len(lon_interp))) + np.nan\n",
    "            for m in np.arange(0, len(sampled[ed]['time'])):\n",
    "                aves[v][month][ed, m, :, :] = interp(sampled[ed], v, m)\n",
    "    return aves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interp(sampled, v, t):\n",
    "    lon_interp = np.arange(-0.5, 0.51, 0.01)\n",
    "    interp_lon = interp1d(sampled[v + '_sec_norm_lon'][t],\n",
    "                          sampled[v + '_sec'][t] - sampled[v + '_around'][t][:,None], axis=1, fill_value=\"extrapolate\")\n",
    "    return interp_lon(lon_interp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testave = prepare(above, ['votemper', 'DIC'], data_int['z'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_interp = np.arange(-0.5, 0.51, 0.01)\n",
    "max_time = 0\n",
    "for ed in np.arange(0, len(above)):\n",
    "    if len(above[ed]['time']) > max_time:\n",
    "        max_time = len(above[ed]['time'])\n",
    "aves = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 'votemper'\n",
    "aves[v] = {}\n",
    "ed = 0\n",
    "month = str(above[ed]['time'][0])[5:7]\n",
    "z = data_int['z'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    aves[v][month] = np.vstack((aves[v][month], np.zeros((1, max_time, len(z), len(lon_interp))) + np.nan))\n",
    "except:\n",
    "    aves[v][month] = np.zeros((1, max_time, len(z), len(lon_interp))) + np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 0\n",
    "aves[v][month][ed, m, :, :] = interp(above[ed], v, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "above[0]['votemper_around'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(testave['votemper']['01'][0,3,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(np.linspace(-1, 1, 101), data['depth_c'].values, np.nanmean(testave['votemper'][:,0,:,:], axis=0),\n",
    "              cmap=cm.balance, vmin=-0.6, vmax=0.6)\n",
    "plt.colorbar()\n",
    "plt.ylim(-1000, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(np.linspace(-1, 1, 101), data['depth_c'].values, np.nanmean(testave['votemper'][:,10,:,:], axis=0),\n",
    "              cmap=cm.balance, vmin=-0.6, vmax=0.6)\n",
    "plt.colorbar()\n",
    "plt.ylim(-1000, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3_eddy]",
   "language": "python",
   "name": "conda-env-py3_eddy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
